{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qsharp\n",
    "import matplotlib.pyplot as plt\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[InstallKernelSpec] Installed kernelspec iqsharp in C:\\ProgramData\\jupyter\\kernels\\iqsharp\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.008365672855715385,0.00949904478879219,0.009392920703974688,0.00943009430094301\\n', '\\n', '0.009202240141286924,0.009392910210146467,0.009195175004943643,0.00956676233429001\\n', '\\n', '0.008133293054167734,0.00949904478879219,0.009392920703974688,0.00943009430094301\\n', '\\n', '0.00801710315339391,0.009605179367437914,0.009244611429701404,0.007926745934126008\\n', '\\n', '0.009062812260358334,0.009286775631500745,0.009195175004943643,0.00970343036763701\\n', '\\n', '0.00848186275648921,0.008809170027594992,0.008947992881154835,0.007790077900779008\\n', '\\n', '0.008295958915251091,0.008968371895563576,0.00914573858018588,0.008610086100861008\\n', '\\n', '0.008737480538191624,0.009021439184886437,0.00914573858018588,0.00984009840098401\\n', '\\n', '0.008458624776334444,0.0090745064742093,0.009096302155428119,0.007790077900779008\\n', '\\n', '0.008249482954941559,0.008596900870303546,0.009640102827763496,0.009156758234249008\\n', '\\n', '0.00834243487556062,0.008809170027594992,0.009392920703974688,0.008336750034167008\\n', '\\n', '0.007784723351846261,0.010082784971343666,0.009392920703974688,0.00984009840098401\\n', '\\n', '0.009202240141286924,0.00912757376353216,0.009689539252521258,0.00970343036763701\\n', '\\n', '0.008249482954941559,0.009286775631500745,0.009392920703974688,0.01011343446767801\\n', '\\n', '0.009504333883298867,0.008915304606240715,0.00944235712873245,0.01011343446767801\\n', '\\n', '0.008412148816024916,0.008543833580980686,0.009244611429701404,0.00970343036763701\\n', '\\n', '0.008040341133548675,0.00912757376353216,0.009343484279216926,0.008746754134208008\\n', '\\n', '0.00852833871679874,0.009976650392697944,0.009244611429701404,0.01038677053437201\\n', '\\n', '0.008667766597727328,0.00944597749946933,0.00944235712873245,0.009156758234249008\\n', '\\n', '0.008574814677108269,0.009870515814052221,0.009343484279216926,0.00956676233429001\\n', '\\n', '0.009039574280203569,0.009976650392697944,0.009392920703974688,0.00984009840098401\\n', '\\n', '0.008295958915251091,0.009552112078115052,0.009986157801067827,0.00970343036763701\\n', '\\n', '0.007900913252620085,0.0090745064742093,0.00914573858018588,0.00929342626759601\\n', '\\n', '0.008412148816024916,0.009180641052855022,0.009244611429701404,0.009020090200902008\\n', '\\n', '0.008853670438965448,0.009870515814052221,0.009392920703974688,0.01011343446767801\\n', '\\n', '0.007691771431227201,0.008543833580980686,0.008799683606881551,0.007926745934126008\\n', '\\n', '0.008133293054167734,0.00949904478879219,0.009491793553490212,0.01018176848435151\\n', '\\n', '0.008760718518346389,0.008490766291657823,0.009046865730670359,0.008405084050840509\\n', '\\n', '0.008807194478655918,0.009870515814052221,0.009541229978247972,0.007995079950799507\\n', '\\n', '0.008969860339739273,0.00912757376353216,0.009837848526794542,0.01025010250102501\\n', '\\n', '0.008853670438965448,0.009021439184886437,0.008947992881154835,0.008678420117534509\\n', '\\n', '0.008853670438965448,0.008756102738272131,0.00978841210203678,0.010455104551045511\\n', '\\n', '0.009225478121441689,0.009392910210146467,0.009541229978247972,0.008746754134208008\\n', '\\n', '0.009202240141286924,0.01098492888983231,0.00944235712873245,0.01066010660106601\\n', '\\n', '0.008969860339739273,0.009021439184886437,0.009294047854459166,0.007926745934126008\\n', '\\n', '0.008295958915251091,0.009021439184886437,0.009343484279216926,0.009156758234249008\\n', '\\n', '0.008412148816024916,0.00912757376353216,0.009244611429701404,0.008610086100861008\\n', '\\n', '0.009341668022215513,0.009021439184886437,0.008700810757366027,0.00943009430094301\\n', '\\n', '0.008179769014477265,0.008437699002334962,0.009195175004943643,0.008336750034167008\\n', '\\n', '0.009016336300048803,0.009339842920823608,0.00944235712873245,0.00895175618422851\\n', '\\n', '0.009062812260358334,0.0090745064742093,0.00944235712873245,0.008336750034167008\\n', '\\n', '0.008946622359584508,0.00949904478879219,0.009392920703974688,0.00908842421757551\\n', '\\n', '0.008551576696953504,0.009817448524729358,0.009541229978247972,0.00956676233429001\\n', '\\n', '0.008853670438965448,0.009339842920823608,0.009244611429701404,0.00936176028426951\\n', '\\n', '0.008272720935096324,0.009286775631500745,0.00944235712873245,0.008678420117534509\\n', '\\n', '0.008598052657263034,0.008756102738272131,0.00914573858018588,0.00929342626759601\\n', '\\n', '0.009341668022215513,0.0090745064742093,0.009541229978247972,0.00929342626759601\\n', '\\n', '0.007459391629679552,0.008225429845043517,0.009294047854459166,0.008336750034167008\\n', '\\n', '0.008667766597727328,0.008915304606240715,0.009491793553490212,0.008200082000820008\\n', '\\n', '0.008505100736643975,0.009764381235406497,0.009096302155428119,0.00949842831761651\\n', '\\n', '0.008365672855715385,0.00944597749946933,0.009640102827763496,0.00943009430094301\\n', '\\n', '0.008365672855715385,0.0090745064742093,0.009244611429701404,0.01011343446767801\\n', '\\n', '0.010805660771965705,0.00949904478879219,0.009491793553490212,0.00956676233429001\\n', '\\n', '0.01055004299026329,0.00992358310337508,0.009294047854459166,0.00963509635096351\\n', '\\n', '0.010503567029953762,0.00944597749946933,0.00978841210203678,0.01079677463441301\\n', '\\n', '0.010712708851346646,0.009658246656760775,0.008799683606881551,0.008883422167555008\\n', '\\n', '0.01068947087119188,0.010029717682020803,0.009640102827763496,0.011343446767801012\\n', '\\n', '0.01082889875212047,0.00944597749946933,0.009541229978247972,0.01038677053437201\\n', '\\n', '0.01092185067273953,0.009180641052855022,0.00914573858018588,0.01011343446767801\\n', '\\n', '0.010666232891037115,0.0090745064742093,0.009392920703974688,0.00977176438431051\\n', '\\n', '0.013478028489763675,0.00944597749946933,0.008947992881154835,0.01011343446767801\\n', '\\n', '0.01078242279181094,0.009870515814052221,0.009392920703974688,0.00943009430094301\\n', '\\n', '0.009852903585620342,0.009180641052855022,0.008947992881154835,0.00984009840098401\\n', '\\n', '0.010038807426858462,0.008809170027594992,0.009244611429701404,0.007926745934126008\\n', '\\n', '0.010852136732275235,0.00949904478879219,0.009640102827763496,0.009020090200902008\\n', '\\n', '0.011735179978156303,0.009764381235406497,0.009887284951552304,0.00929342626759601\\n', '\\n', '0.01078242279181094,0.00944597749946933,0.00944235712873245,0.01011343446767801\\n', '\\n', '0.009504333883298867,0.008809170027594992,0.009244611429701404,0.008746754134208008\\n', '\\n', '0.009876141565775107,0.008862237316917853,0.009244611429701404,0.009156758234249008\\n', '\\n', '0.011038040573513354,0.008915304606240715,0.009837848526794542,0.01066010660106601\\n', '\\n', '0.011061278553668119,0.009711313946083636,0.009640102827763496,0.01052343856771901\\n', '\\n', '0.010898612692584764,0.008809170027594992,0.009491793553490212,0.007380073800738007\\n', '\\n', '0.01073594683150141,0.009286775631500745,0.009244611429701404,0.00997676643433101\\n', '\\n', '0.010573280970418056,0.009021439184886437,0.009689539252521258,0.00956676233429001\\n', '\\n', '0.011828131898775362,0.00949904478879219,0.009689539252521258,0.01004510045100451\\n', '\\n', '0.011642228057537243,0.00949904478879219,0.009392920703974688,0.00929342626759601\\n', '\\n', '0.011572514117072948,0.009180641052855022,0.00978841210203678,0.01004510045100451\\n', '\\n', '0.011177468454441945,0.008703035448949268,0.009837848526794542,0.00908842421757551\\n', '\\n', '0.010619756930727586,0.009180641052855022,0.009541229978247972,0.00984009840098401\\n', '\\n', '0.009876141565775107,0.009180641052855022,0.009244611429701404,0.009156758234249008\\n', '\\n', '0.010503567029953762,0.008809170027594992,0.00944235712873245,0.008883422167555008\\n', '\\n', '0.010596518950572821,0.010295054128635111,0.009590666403005734,0.00963509635096351\\n', '\\n', '0.010875374712429999,0.008756102738272131,0.009343484279216926,0.00997676643433101\\n', '\\n', '0.010619756930727586,0.009021439184886437,0.009640102827763496,0.00997676643433101\\n', '\\n', '0.010108521367322756,0.009605179367437914,0.009986157801067827,0.00929342626759601\\n', '\\n', '0.011665466037692009,0.00992358310337508,0.00978841210203678,0.010318436517698511\\n', '\\n']\n",
      "[[0.008365672855715385, 0.00949904478879219, 0.009392920703974688, 0.00943009430094301], [0.009202240141286924, 0.009392910210146467, 0.009195175004943643, 0.00956676233429001], [0.008133293054167734, 0.00949904478879219, 0.009392920703974688, 0.00943009430094301], [0.00801710315339391, 0.009605179367437914, 0.009244611429701404, 0.007926745934126008], [0.009062812260358334, 0.009286775631500745, 0.009195175004943643, 0.00970343036763701], [0.00848186275648921, 0.008809170027594992, 0.008947992881154835, 0.007790077900779008], [0.008295958915251091, 0.008968371895563576, 0.00914573858018588, 0.008610086100861008], [0.008737480538191624, 0.009021439184886437, 0.00914573858018588, 0.00984009840098401], [0.008458624776334444, 0.0090745064742093, 0.009096302155428119, 0.007790077900779008], [0.008249482954941559, 0.008596900870303546, 0.009640102827763496, 0.009156758234249008], [0.00834243487556062, 0.008809170027594992, 0.009392920703974688, 0.008336750034167008], [0.007784723351846261, 0.010082784971343666, 0.009392920703974688, 0.00984009840098401], [0.009202240141286924, 0.00912757376353216, 0.009689539252521258, 0.00970343036763701], [0.008249482954941559, 0.009286775631500745, 0.009392920703974688, 0.01011343446767801], [0.009504333883298867, 0.008915304606240715, 0.00944235712873245, 0.01011343446767801], [0.008412148816024916, 0.008543833580980686, 0.009244611429701404, 0.00970343036763701], [0.008040341133548675, 0.00912757376353216, 0.009343484279216926, 0.008746754134208008], [0.00852833871679874, 0.009976650392697944, 0.009244611429701404, 0.01038677053437201], [0.008667766597727328, 0.00944597749946933, 0.00944235712873245, 0.009156758234249008], [0.008574814677108269, 0.009870515814052221, 0.009343484279216926, 0.00956676233429001], [0.009039574280203569, 0.009976650392697944, 0.009392920703974688, 0.00984009840098401], [0.008295958915251091, 0.009552112078115052, 0.009986157801067827, 0.00970343036763701], [0.007900913252620085, 0.0090745064742093, 0.00914573858018588, 0.00929342626759601], [0.008412148816024916, 0.009180641052855022, 0.009244611429701404, 0.009020090200902008], [0.008853670438965448, 0.009870515814052221, 0.009392920703974688, 0.01011343446767801], [0.007691771431227201, 0.008543833580980686, 0.008799683606881551, 0.007926745934126008], [0.008133293054167734, 0.00949904478879219, 0.009491793553490212, 0.01018176848435151], [0.008760718518346389, 0.008490766291657823, 0.009046865730670359, 0.008405084050840509], [0.008807194478655918, 0.009870515814052221, 0.009541229978247972, 0.007995079950799507], [0.008969860339739273, 0.00912757376353216, 0.009837848526794542, 0.01025010250102501], [0.008853670438965448, 0.009021439184886437, 0.008947992881154835, 0.008678420117534509], [0.008853670438965448, 0.008756102738272131, 0.00978841210203678, 0.010455104551045511], [0.009225478121441689, 0.009392910210146467, 0.009541229978247972, 0.008746754134208008], [0.009202240141286924, 0.01098492888983231, 0.00944235712873245, 0.01066010660106601], [0.008969860339739273, 0.009021439184886437, 0.009294047854459166, 0.007926745934126008], [0.008295958915251091, 0.009021439184886437, 0.009343484279216926, 0.009156758234249008], [0.008412148816024916, 0.00912757376353216, 0.009244611429701404, 0.008610086100861008], [0.009341668022215513, 0.009021439184886437, 0.008700810757366027, 0.00943009430094301], [0.008179769014477265, 0.008437699002334962, 0.009195175004943643, 0.008336750034167008], [0.009016336300048803, 0.009339842920823608, 0.00944235712873245, 0.00895175618422851], [0.009062812260358334, 0.0090745064742093, 0.00944235712873245, 0.008336750034167008], [0.008946622359584508, 0.00949904478879219, 0.009392920703974688, 0.00908842421757551], [0.008551576696953504, 0.009817448524729358, 0.009541229978247972, 0.00956676233429001], [0.008853670438965448, 0.009339842920823608, 0.009244611429701404, 0.00936176028426951], [0.008272720935096324, 0.009286775631500745, 0.00944235712873245, 0.008678420117534509], [0.008598052657263034, 0.008756102738272131, 0.00914573858018588, 0.00929342626759601], [0.009341668022215513, 0.0090745064742093, 0.009541229978247972, 0.00929342626759601], [0.007459391629679552, 0.008225429845043517, 0.009294047854459166, 0.008336750034167008], [0.008667766597727328, 0.008915304606240715, 0.009491793553490212, 0.008200082000820008], [0.008505100736643975, 0.009764381235406497, 0.009096302155428119, 0.00949842831761651], [0.008365672855715385, 0.00944597749946933, 0.009640102827763496, 0.00943009430094301], [0.008365672855715385, 0.0090745064742093, 0.009244611429701404, 0.01011343446767801], [0.010805660771965705, 0.00949904478879219, 0.009491793553490212, 0.00956676233429001], [0.01055004299026329, 0.00992358310337508, 0.009294047854459166, 0.00963509635096351], [0.010503567029953762, 0.00944597749946933, 0.00978841210203678, 0.01079677463441301], [0.010712708851346646, 0.009658246656760775, 0.008799683606881551, 0.008883422167555008], [0.01068947087119188, 0.010029717682020803, 0.009640102827763496, 0.011343446767801012], [0.01082889875212047, 0.00944597749946933, 0.009541229978247972, 0.01038677053437201], [0.01092185067273953, 0.009180641052855022, 0.00914573858018588, 0.01011343446767801], [0.010666232891037115, 0.0090745064742093, 0.009392920703974688, 0.00977176438431051], [0.013478028489763675, 0.00944597749946933, 0.008947992881154835, 0.01011343446767801], [0.01078242279181094, 0.009870515814052221, 0.009392920703974688, 0.00943009430094301], [0.009852903585620342, 0.009180641052855022, 0.008947992881154835, 0.00984009840098401], [0.010038807426858462, 0.008809170027594992, 0.009244611429701404, 0.007926745934126008], [0.010852136732275235, 0.00949904478879219, 0.009640102827763496, 0.009020090200902008], [0.011735179978156303, 0.009764381235406497, 0.009887284951552304, 0.00929342626759601], [0.01078242279181094, 0.00944597749946933, 0.00944235712873245, 0.01011343446767801], [0.009504333883298867, 0.008809170027594992, 0.009244611429701404, 0.008746754134208008], [0.009876141565775107, 0.008862237316917853, 0.009244611429701404, 0.009156758234249008], [0.011038040573513354, 0.008915304606240715, 0.009837848526794542, 0.01066010660106601], [0.011061278553668119, 0.009711313946083636, 0.009640102827763496, 0.01052343856771901], [0.010898612692584764, 0.008809170027594992, 0.009491793553490212, 0.007380073800738007], [0.01073594683150141, 0.009286775631500745, 0.009244611429701404, 0.00997676643433101], [0.010573280970418056, 0.009021439184886437, 0.009689539252521258, 0.00956676233429001], [0.011828131898775362, 0.00949904478879219, 0.009689539252521258, 0.01004510045100451], [0.011642228057537243, 0.00949904478879219, 0.009392920703974688, 0.00929342626759601], [0.011572514117072948, 0.009180641052855022, 0.00978841210203678, 0.01004510045100451], [0.011177468454441945, 0.008703035448949268, 0.009837848526794542, 0.00908842421757551], [0.010619756930727586, 0.009180641052855022, 0.009541229978247972, 0.00984009840098401], [0.009876141565775107, 0.009180641052855022, 0.009244611429701404, 0.009156758234249008], [0.010503567029953762, 0.008809170027594992, 0.00944235712873245, 0.008883422167555008], [0.010596518950572821, 0.010295054128635111, 0.009590666403005734, 0.00963509635096351], [0.010875374712429999, 0.008756102738272131, 0.009343484279216926, 0.00997676643433101], [0.010619756930727586, 0.009021439184886437, 0.009640102827763496, 0.00997676643433101], [0.010108521367322756, 0.009605179367437914, 0.009986157801067827, 0.00929342626759601], [0.011665466037692009, 0.00992358310337508, 0.00978841210203678, 0.010318436517698511]]\n",
      "['0.009179002161132158,0.009233708342177883,0.009195175004943643,0.01038677053437201\\n', '\\n', '0.009364906002370277,0.009552112078115052,0.009640102827763496,0.008883422167555008\\n', '\\n', '0.00852833871679874,0.01024198683931225,0.009541229978247972,0.00943009430094301\\n', '\\n', '0.009039574280203569,0.00944597749946933,0.008947992881154835,0.00990843241765751\\n', '\\n', '0.009550809843608397,0.009339842920823608,0.008997429305912597,0.008746754134208008\\n', '\\n', '0.008505100736643975,0.00944597749946933,0.00914573858018588,0.01011343446767801\\n', '\\n', '0.008993098319894038,0.010082784971343666,0.009640102827763496,0.00943009430094301\\n', '\\n', '0.007993865173239145,0.009764381235406497,0.009096302155428119,0.00908842421757551\\n', '\\n', '0.008783956498501153,0.009711313946083636,0.008601937907850504,0.00929342626759601\\n', '\\n', '0.00834243487556062,0.010188919549989389,0.009343484279216926,0.01038677053437201\\n', '\\n', '0.008203006994632028,0.010029717682020803,0.009244611429701404,0.01038677053437201\\n', '\\n', '0.009411381962679807,0.00949904478879219,0.009244611429701404,0.008746754134208008\\n', '\\n', '0.008807194478655918,0.009870515814052221,0.008503065058334982,0.008610086100861008\\n', '\\n', '0.009179002161132158,0.008862237316917853,0.008799683606881551,0.008883422167555008\\n', '\\n', '0.009179002161132158,0.00944597749946933,0.009294047854459166,0.009020090200902008\\n', '\\n', '0.008458624776334444,0.009021439184886437,0.009640102827763496,0.00908842421757551\\n', '\\n', '0.009806427625310813,0.009817448524729358,0.008898556456397073,0.00970343036763701\\n', '\\n', '0.008737480538191624,0.01024198683931225,0.008947992881154835,0.009020090200902008\\n', '\\n', '0.00848186275648921,0.009552112078115052,0.008997429305912597,0.008610086100861008\\n', '\\n', '0.008365672855715385,0.009817448524729358,0.009195175004943643,0.008473418067514008\\n', '\\n', '0.008598052657263034,0.008968371895563576,0.00914573858018588,0.008200082000820008\\n', '\\n']\n",
      "[[0.009179002161132158, 0.009233708342177883, 0.009195175004943643, 0.01038677053437201], [0.009364906002370277, 0.009552112078115052, 0.009640102827763496, 0.008883422167555008], [0.00852833871679874, 0.01024198683931225, 0.009541229978247972, 0.00943009430094301], [0.009039574280203569, 0.00944597749946933, 0.008947992881154835, 0.00990843241765751], [0.009550809843608397, 0.009339842920823608, 0.008997429305912597, 0.008746754134208008], [0.008505100736643975, 0.00944597749946933, 0.00914573858018588, 0.01011343446767801], [0.008993098319894038, 0.010082784971343666, 0.009640102827763496, 0.00943009430094301], [0.007993865173239145, 0.009764381235406497, 0.009096302155428119, 0.00908842421757551], [0.008783956498501153, 0.009711313946083636, 0.008601937907850504, 0.00929342626759601], [0.00834243487556062, 0.010188919549989389, 0.009343484279216926, 0.01038677053437201], [0.008203006994632028, 0.010029717682020803, 0.009244611429701404, 0.01038677053437201], [0.009411381962679807, 0.00949904478879219, 0.009244611429701404, 0.008746754134208008], [0.008807194478655918, 0.009870515814052221, 0.008503065058334982, 0.008610086100861008], [0.009179002161132158, 0.008862237316917853, 0.008799683606881551, 0.008883422167555008], [0.009179002161132158, 0.00944597749946933, 0.009294047854459166, 0.009020090200902008], [0.008458624776334444, 0.009021439184886437, 0.009640102827763496, 0.00908842421757551], [0.009806427625310813, 0.009817448524729358, 0.008898556456397073, 0.00970343036763701], [0.008737480538191624, 0.01024198683931225, 0.008947992881154835, 0.009020090200902008], [0.00848186275648921, 0.009552112078115052, 0.008997429305912597, 0.008610086100861008], [0.008365672855715385, 0.009817448524729358, 0.009195175004943643, 0.008473418067514008], [0.008598052657263034, 0.008968371895563576, 0.00914573858018588, 0.008200082000820008]]\n",
      "86\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "X_train=open('C:/Users\\ceqcx\\Downloads/0val_train')\n",
    "X_train=X_train.readlines()\n",
    "print(X_train)\n",
    "X_train_new=[]\n",
    "for i in X_train:\n",
    "    if i != '\\n':\n",
    "        X_train_new.append(i)\n",
    "for i in range(len(X_train_new)):\n",
    "    X_train_new[i]=X_train_new[i].replace('\\n',\"\")\n",
    "    X_train_new[i]=X_train_new[i].split(',')\n",
    "for i in range(len(X_train_new)):\n",
    "    for j in range(len(X_train_new[i])):\n",
    "            X_train_new[i][j]=float(X_train_new[i][j])\n",
    "print(X_train_new)\n",
    "X_train=X_train_new\n",
    "X_test=open('C:/Users\\ceqcx\\Downloads/0val_test')\n",
    "X_test=X_test.readlines()\n",
    "print(X_test)\n",
    "X_test_new=[]\n",
    "for i in X_test:\n",
    "    if i != '\\n':\n",
    "        X_test_new.append(i)\n",
    "for i in range(len(X_test_new)):\n",
    "    X_test_new[i]=X_test_new[i].replace('\\n',\"\")\n",
    "    X_test_new[i]=X_test_new[i].split(',')\n",
    "for i in range(len(X_test_new)):\n",
    "    for j in range(len(X_test_new[i])):\n",
    "            X_test_new[i][j]=float(X_test_new[i][j])\n",
    "print(X_test_new)\n",
    "X_test=X_test_new\n",
    "Y_train=open('C:/Users\\ceqcx\\Downloads/0check_train')\n",
    "Y_train=Y_train.readlines()\n",
    "Y_train=Y_train[0].split(',')\n",
    "for i in range(len(Y_train)):\n",
    "     Y_train[i]=int(Y_train[i])\n",
    "print(len(Y_train))\n",
    "\n",
    "Y_test=open('C:/Users\\ceqcx\\Downloads/0check_test')\n",
    "Y_test=Y_test.readlines()\n",
    "Y_test=Y_test[0].split(',')\n",
    "for i in range(len(Y_test)):\n",
    "     Y_test[i]=int(Y_test[i])\n",
    "print(len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%qsharp\n",
    "\n",
    "open Microsoft.Quantum.Math;\n",
    "open Microsoft.Quantum.Arrays;\n",
    "\n",
    "operation ZZFeatureMap(qs : Qubit[], data : Double[]) : Unit {\n",
    "    let numberOfQubits = Length(qs);\n",
    "    if (numberOfQubits != Length(data)) {\n",
    "        fail \"Data length must be equal to the number of qubits.\";\n",
    "    }\n",
    "    for i in 0..numberOfQubits-1 {\n",
    "        // Apply Z rotation to each qubit\n",
    "        Rz(2.0 * PI() * data[i], qs[i]);\n",
    "    }\n",
    "    for i in 0..numberOfQubits-1 {\n",
    "        for j in i+1..numberOfQubits-1 {\n",
    "            // Apply ZZ interaction between each pair of qubits\n",
    "            let theta = 2.0 * PI() * data[i] * data[j];\n",
    "            CNOT(qs[i], qs[j]);\n",
    "            Rz(theta, qs[j]);\n",
    "            CNOT(qs[i], qs[j]);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "operation RealAmplitudes(qubits : Qubit[], parameters : Double[]) : Unit {\n",
    "    // Apply layer of rotation gates\n",
    "    for idx in IndexRange(qubits) {\n",
    "        Ry(parameters[idx], qubits[idx]);\n",
    "    }\n",
    "\n",
    "    // Apply a layer of CNOT gates\n",
    "    for idx in 0 .. Length(qubits)-2 {\n",
    "        for i in idx+1 .. Length(qubits)-1{\n",
    "            CNOT(qubits[idx], qubits[i]);\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    // Apply another layer of rotation gates\n",
    "    for idx in IndexRange(qubits) {\n",
    "        Ry(parameters[idx + Length(qubits)], qubits[idx]);\n",
    "    }\n",
    "}\n",
    "\n",
    "operation quantumVariationalClassifier (qubits: Qubit[], x : Double[], θ : Double[]) : Unit {\n",
    "    ZZFeatureMap(qubits, x);\n",
    "    RealAmplitudes(qubits, θ);\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "QSharpCallable.__call__() takes 1 positional argument but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(steps):\n\u001b[1;32m---> 16\u001b[0m     gradients \u001b[39m=\u001b[39m qml\u001b[39m.\u001b[39;49mgrad(cost, argnum\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)(weights, X_train_scaled, Y_train)\n\u001b[0;32m     18\u001b[0m     \u001b[39m# Update the weights\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     optimizer\u001b[39m.\u001b[39mstep(gradients, weights)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pennylane\\_grad.py:120\u001b[0m, in \u001b[0;36mgrad.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fun(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m     \u001b[39mreturn\u001b[39;00m ()\n\u001b[1;32m--> 120\u001b[0m grad_value, ans \u001b[39m=\u001b[39m grad_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward \u001b[39m=\u001b[39m ans\n\u001b[0;32m    123\u001b[0m \u001b[39mreturn\u001b[39;00m grad_value\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\autograd\\wrap_util.py:20\u001b[0m, in \u001b[0;36munary_to_nary.<locals>.nary_operator.<locals>.nary_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(args[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m argnum)\n\u001b[1;32m---> 20\u001b[0m \u001b[39mreturn\u001b[39;00m unary_operator(unary_f, x, \u001b[39m*\u001b[39mnary_op_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnary_op_kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pennylane\\_grad.py:138\u001b[0m, in \u001b[0;36mgrad._grad_with_forward\u001b[1;34m(fun, x)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    133\u001b[0m \u001b[39m@unary_to_nary\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_grad_with_forward\u001b[39m(fun, x):\n\u001b[0;32m    135\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"This function is a replica of ``autograd.grad``, with the only\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m    difference being that it returns both the gradient *and* the forward pass\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[39m    value.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m     vjp, ans \u001b[39m=\u001b[39m _make_vjp(fun, x)\n\u001b[0;32m    140\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vspace(ans)\u001b[39m.\u001b[39msize \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    141\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    142\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mGrad only applies to real scalar-output functions. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    143\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mTry jacobian, elementwise_grad or holomorphic_grad.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    144\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\autograd\\core.py:10\u001b[0m, in \u001b[0;36mmake_vjp\u001b[1;34m(fun, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_vjp\u001b[39m(fun, x):\n\u001b[0;32m      9\u001b[0m     start_node \u001b[39m=\u001b[39m VJPNode\u001b[39m.\u001b[39mnew_root()\n\u001b[1;32m---> 10\u001b[0m     end_value, end_node \u001b[39m=\u001b[39m  trace(start_node, fun, x)\n\u001b[0;32m     11\u001b[0m     \u001b[39mif\u001b[39;00m end_node \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m         \u001b[39mdef\u001b[39;00m \u001b[39mvjp\u001b[39m(g): \u001b[39mreturn\u001b[39;00m vspace(x)\u001b[39m.\u001b[39mzeros()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\autograd\\tracer.py:10\u001b[0m, in \u001b[0;36mtrace\u001b[1;34m(start_node, fun, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mwith\u001b[39;00m trace_stack\u001b[39m.\u001b[39mnew_trace() \u001b[39mas\u001b[39;00m t:\n\u001b[0;32m      9\u001b[0m     start_box \u001b[39m=\u001b[39m new_box(x, t, start_node)\n\u001b[1;32m---> 10\u001b[0m     end_box \u001b[39m=\u001b[39m fun(start_box)\n\u001b[0;32m     11\u001b[0m     \u001b[39mif\u001b[39;00m isbox(end_box) \u001b[39mand\u001b[39;00m end_box\u001b[39m.\u001b[39m_trace \u001b[39m==\u001b[39m start_box\u001b[39m.\u001b[39m_trace:\n\u001b[0;32m     12\u001b[0m         \u001b[39mreturn\u001b[39;00m end_box\u001b[39m.\u001b[39m_value, end_box\u001b[39m.\u001b[39m_node\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\autograd\\wrap_util.py:15\u001b[0m, in \u001b[0;36munary_to_nary.<locals>.nary_operator.<locals>.nary_f.<locals>.unary_f\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     subargs \u001b[39m=\u001b[39m subvals(args, \u001b[39mzip\u001b[39m(argnum, x))\n\u001b[1;32m---> 15\u001b[0m \u001b[39mreturn\u001b[39;00m fun(\u001b[39m*\u001b[39msubargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m, in \u001b[0;36mcost\u001b[1;34m(weights, data, labels)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcost\u001b[39m(weights, data, labels):\n\u001b[0;32m      2\u001b[0m     \u001b[39m# Make predictions using the quantum neural network, labels are the expected/real classification for the data\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     predictions \u001b[39m=\u001b[39m quantumVariationalClassifier([], data, weights)\n\u001b[0;32m      4\u001b[0m     mse \u001b[39m=\u001b[39m qml\u001b[39m.\u001b[39mmean_squared_error(labels, predictions)\n\u001b[0;32m      5\u001b[0m     \u001b[39mreturn\u001b[39;00m mse\n",
      "\u001b[1;31mTypeError\u001b[0m: QSharpCallable.__call__() takes 1 positional argument but 4 were given"
     ]
    }
   ],
   "source": [
    "def cost(weights, data, labels):\n",
    "    # Make predictions using the quantum neural network, labels are the expected/real classification for the data\n",
    "    predictions = quantumVariationalClassifier([], data, weights)\n",
    "    mse = qml.mean_squared_error(labels, predictions)\n",
    "    return mse\n",
    "\n",
    "optimizer = qml.AdamOptimizer(stepsize=0.01)\n",
    "\n",
    "steps = 100\n",
    "\n",
    "#probably need to make this into a list or array to match theta type in other functions\n",
    "weights = np.random.normal(0, 1, (4, 2))\n",
    "\n",
    "# Train the model\n",
    "for i in range(steps):\n",
    "    gradients = qml.grad(cost, argnum=0)(weights, X_train_scaled, Y_train)\n",
    "\n",
    "    # Update the weights\n",
    "    optimizer.step(gradients, weights)\n",
    "\n",
    "    # Print the cost\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f'Step {i + 1}: cost = {cost(weights, X_train_scaled, Y_train):.4f}')\n",
    "\n",
    "predictions = quantumVariationalClassifier(weights, X_test_scaled)\n",
    "\n",
    "accuracy = qml.accuracy(predictions, y_test)\n",
    "\n",
    "print(f'Test accuracy: {accuracy:.2f}')\n",
    "\n",
    "plt.scatter(y_test, predictions)\n",
    "\n",
    "# Add a diagonal line\n",
    "x = np.linspace(0, 3, 4)\n",
    "plt.plot(x, x, '--r')\n",
    "\n",
    "# Add axis labels and a title\n",
    "plt.xlabel('True labels')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('Quantum Neural Network')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
